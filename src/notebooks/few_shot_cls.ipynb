{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ceee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from queue import Queue\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from gemini_api import GeminiAPIHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "dataset = \"derm12345\"\n",
    "dataset_path = f\"../../datasets/{dataset}\"\n",
    "\n",
    "hierarchy_path = f\"../../datasets/{dataset}/dataset_hierarchy.json\"\n",
    "with open(hierarchy_path, 'r') as dh:\n",
    "    derm12345_hier = json.load(dh)\n",
    "\n",
    "metadata_path = f\"../../datasets/{dataset}/metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "output_dir = \"../api_cls/derm12345\"\n",
    "os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "\n",
    "log_filename = os.path.join(output_dir, f\"api_clscont.log\")\n",
    "os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "    filename=log_filename\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "eval_dir = \"../api_cls/derm12345/evaluation_metrics\"\n",
    "os.makedirs(os.path.dirname(eval_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c963f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataset directory\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path, topdown=False):\n",
    "        if dirpath == dataset_path:\n",
    "            continue  # Skip the root itself\n",
    "\n",
    "        for file in filenames:\n",
    "            src_path = os.path.join(dirpath, file)\n",
    "            dest_path = os.path.join(dataset_path, file)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "        # Optionally remove the now-empty subfolders\n",
    "        os.rmdir(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eebe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "num_key = 5\n",
    "api_handler_queue = Queue()\n",
    "model_name = \"gemini-2.0-flash-exp\"\n",
    "#model_name = \"gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "for index in range(1, num_key + 1):\n",
    "    api_key = os.getenv(f\"GEMINI_API_KEY_{index}\")\n",
    "    api_handler = GeminiAPIHandler(api_key=api_key, index=index, model_name=model_name)\n",
    "    api_handler_queue.put(api_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "        f\"***** SYSTEM *****\\n\"\n",
    "        f\"You are a dermatologist examining a lesion/skin disease image.\\n\"\n",
    "        f\"The image is from the DERM12345 dataset and the dataset is organised hierarchically:\\n\"\n",
    "         \n",
    "        f\"• **5 super-classes** (broad diagnostic families)\\n\"\n",
    "        f\"• **15 main classes** (finer diagnostic groups inside each super-class)\\n\"\n",
    "        f\"• **40 subclasses** (leaf-level, mutually-exclusive labels used for evaluation)\\n\"\n",
    "\n",
    "        f\"• Besides, the `label` field is a unique identifier assigned to each subclass\\n\"\n",
    "\n",
    "        f\"***** GOAL *****\\n\" \n",
    "        f\"Determine which **subclass** it belongs to based on the **DATASET HIERARCHY** provided below.\" \n",
    "\n",
    "        f\"***** DATASET HIERARCHY *****\\n\"\n",
    "        f\"{derm12345_hier}\\n\\n\"\n",
    "\n",
    "        f\"***** RULES *****\\n\"  \n",
    "        f\"1. Traverse the dataset hierarchy and determine the correct classification path for the input image.\\n\"\n",
    "        f\"2. Output **exactly one line** in the following format:\\n\"\n",
    "        f\"   <SuperClass> - <MainClass> - <SubClass> - <Label>\\n\"\n",
    "        f\"3. The values must be in lower case and match the class names and labels from the **Dataset HIERARCHY** exactly.\\n\"\n",
    "        f\"4. Do **not** include quotes, extra punctuation, or reasoning.\\n\"\n",
    "        f\"5. If the image does **not** match any super-class, main class or subclass with ≥ 90 % confidence, output **\\\"unknow\\\"** in place.\\n\"\n",
    "        f\"   • If super-class undetermined, output \\\"unknown - unkown - unknown - unknown\\\";\\n\"\n",
    "        f\"   • Else if main class undetermined, output \\\"<SuperClass> - unkown - unknown - unknown\\\";\\n\"\n",
    "        f\"   • Else if subclass undetermined, output \\\"<SuperClass> - <MainClass> - unknown - unknown\\\";\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_api_cls(api_handler: GeminiAPIHandler, image_id: str, metadata) -> dict:\n",
    "    \"\"\"\n",
    "    Call the Gemini API for a given request and dataset image path.\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(dataset_path, image_id + \".jpg\")\n",
    "    \n",
    "    # Load the image\n",
    "    with Image.open(image_path) as pil_image:\n",
    "        # Generate response\n",
    "        response = api_handler.generate_from_pil_image(pil_image, prompt)\n",
    "    \n",
    "    parts = response.rstrip(\"\\n\").split(\" - \")\n",
    "    if len(parts) == 4:\n",
    "        super_class_pred = parts[0].lower()\n",
    "        main_class_pred = super_class_pred + \" - \" + parts[1].lower()\n",
    "        label_pred = parts[3].lower()\n",
    "    else:\n",
    "        super_class_pred = \"malformed output\"\n",
    "        main_class_pred = \"malformed output\"\n",
    "        label_pred = \"malformed output\"\n",
    "\n",
    "    row = metadata[metadata['image_id'] == image_id]\n",
    "    super_class_true = row['super_class'].values[0] + \" \" + row['malignancy'].values[0]\n",
    "    if row['main_class_1'].values[0] == row['main_class_2'].values[0]:\n",
    "        main_class_true = super_class_true + \" - \" + row['main_class_1'].values[0]\n",
    "    else:\n",
    "        main_class_true = super_class_true + \" - \" + row['main_class_1'].values[0] + \" \" + row['main_class_2'].values[0]\n",
    "    label_true = row['label'].values[0]\n",
    "    \n",
    "    result = {\n",
    "        \"image_id\": image_id,\n",
    "        \"image_path\": image_path,\n",
    "        \"response\": response,\n",
    "        \"super_class_pred\": super_class_pred,\n",
    "        \"super_class_true\": super_class_true,\n",
    "        \"main_class_pred\": main_class_pred,\n",
    "        \"main_class_true\": main_class_true,\n",
    "        \"label_pred\": label_pred,\n",
    "        \"label_true\": label_true\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_api_handler(toggle_reason):\n",
    "    api_handler_queue.put(api_handler) # Requeue the old API handler\n",
    "    api_handler = api_handler_queue.get() # Get the new API handler\n",
    "    logger.info(f\"{toggle_reason}. Switch to API_Handler_{api_handler.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb66c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_api_result(result, super_true, super_pred, main_true, main_pred, sub_true, sub_pred):\n",
    "    super_true.append(result['super_class_true'])\n",
    "    super_pred.append(result['super_class_pred'])\n",
    "    main_true.append(result['main_class_true'])\n",
    "    main_pred.append(result['main_class_pred'])\n",
    "    sub_true.append(result['label_true'])\n",
    "    sub_pred.append(result['label_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_overall(y_true, y_pred, granularity, isprocessing):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    eval_metrics = {\n",
    "        'Granularity': f'{granularity}',\n",
    "        'Accuracy': f'{accuracy:.3%}',\n",
    "        'Sensitivity': f'{sensitivity:.3%}'\n",
    "    }\n",
    "\n",
    "    message = f\"• **{granularity} Level: Accuracy={accuracy:.3%}, Sensitivity={sensitivity:.3%}\"\n",
    "    if isprocessing:\n",
    "        tqdm.write(message) # Temporarily overwrite the progress bar\n",
    "    else:\n",
    "        print(message)\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_per_class(y_true, y_pred, granularity):   \n",
    "    labels = sorted(set(y_true))  # all unique labels in ground truth\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    sensitivities = recall_score(y_true, y_pred, average=None, labels=labels)\n",
    "    specificities = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        TP = cm[i, i]\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        specificities.append(specificity)\n",
    "\n",
    "    eval_data = {\n",
    "        'Class': labels,\n",
    "        'Sensitivity': sensitivities,\n",
    "        'Specificity': specificities\n",
    "    }\n",
    "\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    display(eval_df)\n",
    "    eval_file_path = os.path.join(eval_dir, f\"per_{granularity}_class\")\n",
    "    eval_df.to_csv(eval_file_path, index=False)\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83babc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_queue = Queue()\n",
    "for idx, row in metadata.iterrows():\n",
    "    img_queue.put(row['image_id'])\n",
    "img_total = img_queue.qsize()\n",
    "\n",
    "super_true = []\n",
    "super_pred = []\n",
    "main_true = []\n",
    "main_pred = []\n",
    "sub_true = []\n",
    "sub_pred = []\n",
    "\n",
    "# Parse all existing response files in the output directory\n",
    "existing_response_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
    "for filename in existing_response_files:\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with open(file_path, 'r') as f:\n",
    "        stored_result = json.load(f)\n",
    "    parse_api_result(stored_result, super_true, super_pred, main_true, main_pred, sub_true, sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a tqdm progress bar\n",
    "progress_bar = tqdm(total=img_total, desc=\"Processing images\", unit=\"img\")\n",
    "progress_bar.n = len(existing_response_files)\n",
    "progress_bar.refresh()\n",
    "\n",
    "# Monitor the progress\n",
    "now = time.time()\n",
    "latest_update = now\n",
    "\n",
    "api_handler = api_handler_queue.get()\n",
    "handler_task_count = 0\n",
    "\n",
    "while not img_queue.empty():\n",
    "    image_id = img_queue.get()\n",
    "    task_exists = False\n",
    "    progress_made = False\n",
    "    try:\n",
    "        generated_response_file = os.path.join(output_dir, f\"{image_id}_cls.json\")\n",
    "        \n",
    "        if os.path.exists(generated_response_file):\n",
    "            logger.info(f\"Skipping image_id {image_id} as response already exists.\")\n",
    "            task_exists = True\n",
    "\n",
    "        else:\n",
    "            result = call_gemini_api_cls(api_handler, image_id, metadata)\n",
    "            parse_api_result(result, super_true, super_pred, main_true, main_pred, sub_true, sub_pred)\n",
    "\n",
    "            with open(generated_response_file, \"w\") as out_file:\n",
    "                json.dump(result, out_file, indent=4)\n",
    "            logger.info(f\"Successfully processed image_id {image_id}, saved to {generated_response_file}\")\n",
    "            progress_made = True\n",
    "            handler_task_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing image_id {image_id}: {e}\")\n",
    "        \n",
    "        # Requeue the request\n",
    "        img_queue.put(image_id)\n",
    "\n",
    "    finally:\n",
    "        img_queue.task_done()\n",
    "\n",
    "        now = time.time()\n",
    "        if (now - latest_update > 180):\n",
    "            toggle_api_handler(\"API request timeout\")\n",
    "        elif handler_task_count >= 100:\n",
    "            toggle_api_handler(\"API_Handler_{api_handler.index} sleeps\")\n",
    "            handler_task_count = 0\n",
    "\n",
    "        if not task_exists and progress_made:    \n",
    "            progress_bar.update(1)\n",
    "            if progress_bar.n % 50 == 0: \n",
    "                eval_overall(super_true, super_pred, \"SuperClass\", isprocessing=True)\n",
    "                eval_overall(main_true, main_pred, \"MainClass\", isprocessing=True)\n",
    "                eval_overall(sub_true, sub_pred, \"SubClass\", isprocessing=True)\n",
    "                latest_update = now\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1431728",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_response_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
    "print(f\"{len(existing_response_files)}/{img_total} of the images have been processed so far.\\n\")\n",
    "super_overall = eval_overall(super_true, super_pred, \"SuperClass\", isprocessing=False)\n",
    "main_overall = eval_overall(main_true, main_pred, \"MainClass\", isprocessing=False)\n",
    "sub_overall = eval_overall(sub_true, sub_pred, \"SubClass\", isprocessing=False)\n",
    "\n",
    "overall_eval = pd.DataFrame([super_overall, main_overall, sub_overall])\n",
    "overall_eval_file = os.path.join(eval_dir, f\"overall_evaluation.csv\")\n",
    "os.makedirs(os.path.dirname(overall_eval_file), exist_ok=True)\n",
    "overall_eval.to_csv(overall_eval_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_super_class = eval_per_class(super_true, super_pred, 'super')\n",
    "per_main_class = eval_per_class(main_true, main_pred, 'main')\n",
    "per_sub_class = eval_per_class(sub_true, sub_pred, 'sub')\n",
    "\n",
    "\n",
    "# Orgnise the evaluation files hierarchically\n",
    "for super_class_key, super_class_dict in derm12345_hier.items():\n",
    "    one_super_eval = per_super_class[per_super_class['Class'] == super_class_key]\n",
    "\n",
    "    for main_class_key, main_class_dict in super_class_dict.items():\n",
    "        main_class_key = super_class_key + \" - \" + main_class_key\n",
    "        one_main_eval = per_main_class[per_main_class['Class'] == main_class_key]\n",
    "        one_super_eval = pd.concat([one_super_eval, one_main_eval], ignore_index=True)\n",
    "\n",
    "        for subclass in main_class_dict['subclasses']:\n",
    "            one_sub_eval = per_sub_class[per_sub_class['Class'] == subclass.get('label')]\n",
    "            one_main_eval = pd.concat([one_main_eval, one_sub_eval], ignore_index=True)\n",
    "        \n",
    "        one_main_eval_file = os.path.join(eval_dir, f\"{main_class_key}.csv\")\n",
    "        one_main_eval.to_csv(one_main_eval_file, index=False)\n",
    "\n",
    "    one_super_eval_file = os.path.join(eval_dir, f\"{super_class_key}.csv\")\n",
    "    one_super_eval.to_csv(one_super_eval_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
